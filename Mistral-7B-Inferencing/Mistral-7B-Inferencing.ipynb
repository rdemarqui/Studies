{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHDnWyp5GJVd"
      },
      "source": [
        "##  Inferencing on Mistral 7B LLM with 4-bit quantization 🚀 - In FREE Google Colab\n",
        "\n",
        "# [Link to my Youtube Video Explaining this whole Notebook](https://www.youtube.com/watch?v=eovBbABk3hw&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=10&ab_channel=Rohan-Paul-AI)\n",
        "\n",
        "[![Imgur](https://imgur.com/Lz4ov4K.png)](https://www.youtube.com/watch?v=eovBbABk3hw&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=10&ab_channel=Rohan-Paul-AI)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WqidwNaaGJVh",
        "outputId": "4adb5e2b-f51a-4626-ea9c-e4e55ef090ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers -q peft  accelerate bitsandbytes safetensors sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVqy1hXqGJVj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = 'bn22/Mistral-7B-Instruct-v0.1-sharded'\n",
        "\n",
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    :param model_name: Name or path of the model to be loaded.\n",
        "    :return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    Initialize the tokenizer with the specified model_name.\n",
        "\n",
        "    :param model_name: Name or path of the model for tokenizer initialization.\n",
        "    :return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "model = load_quantized_model(model_name)\n",
        "\n",
        "tokenizer = initialize_tokenizer(model_name)\n",
        "\n",
        "# Define stop token ids\n",
        "stop_token_ids = [0]\n",
        "\n",
        "text = \"[INST] How AI will replace Engineers [/INST]\"\n",
        "\n",
        "encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_input = encoded\n",
        "generated_ids = model.generate(**model_input, max_new_tokens=200, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "O texto a seguir é uma transcrição de atendimento:\n",
        "\n",
        "Atendente: Obrigado por ligar para o Serviço de Atendimento ao Cliente da ABC Eletrônicos. Meu nome é Maria. Como posso ajudar você hoje?\n",
        "Cliente: Olá, Maria. Meu nome é João, e tenho enfrentado muitos problemas com o produto que comprei da sua empresa. Comprei um smartphone modelo ABC123 há algumas semanas, e ele só tem me dado problemas.\n",
        "Atendente: Lamento ouvir isso, João. Entendo como isso pode ser frustrante. Você poderia descrever os problemas que está enfrentando com o seu smartphone ABC123?\n",
        "Cliente: Bem, a vida útil da bateria é terrível, e o aparelho reinicia aleatoriamente. A qualidade da câmera não corresponde ao que foi anunciado, e ele trava às vezes. Estou realmente decepcionado com essa compra.\n",
        "Atendente: Peço desculpas pela inconveniência que você está enfrentando. Levamos essas questões a sério, e gostaria de ajudar a resolver esses problemas. Vamos começar verificando algumas coisas. Você tentou reiniciar o telefone ou realizar uma restauração de fábrica?\n",
        "Cliente: Sim, tentei essas etapas, e os problemas persistem. Atualizei o software também, mas não resolveu.\n",
        "Atendente: Obrigada por tentar essas etapas, João. Como o seu telefone ainda está na garantia, podemos enviar a você um aparelho de substituição ou agendar um técnico para avaliar e reparar o seu telefone atual. Qual opção você prefere?\n",
        "Cliente: Acho que gostaria de optar pela substituição, Maria. Tive muitos problemas com esse telefone, e preciso de um aparelho confiável.\n",
        "Atendente: Entendi, João. Vou processar o pedido de substituição para você. Deve chegar nos próximos dias úteis. Também vou enviar a você os detalhes de envio e as instruções para devolver o seu telefone atual por e-mail.\n",
        "Cliente: Obrigado, Maria. Agradeço sua ajuda na resolução deste problema. Espero que o aparelho de substituição funcione melhor.\n",
        "Atendente: De nada, João. Estamos aqui para garantir a satisfação dos nossos clientes. Se você tiver algum problema com a substituição ou tiver outras preocupações, não hesite em nos contatar. Há mais alguma coisa em que posso ajudar hoje?\n",
        "Cliente: Não, é só isso por enquanto. Obrigado pela sua assistência, Maria.\n",
        "Atendente: Foi um prazer, João. Tenha um ótimo dia, e esperamos que o novo telefone atenda às suas expectativas.\n",
        "\n",
        "Identifique as reclamações do cliente.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "b38_r1VNHvE5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_input = encoded\n",
        "generated_ids = model.generate(**model_input, max_new_tokens=400, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "AJvwOI9nG0wo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}